# Data Extraction in Spark


## Pure SQL

Target dataset: [TPCH](../../resources/dataset/tpch_100_query_traces) and [TPCDS](../../resources/dataset/tpcds_100_query_traces).

1. get operator features via `w2v` and `d2v`
```bash
export PYTHONPATH="$PWD"
# via word2vec tf-idf
python examples/data/spark/1.operator_encoding.py -b TPCH --scale-factor 100 \
--src-path-header resources/dataset/tpch_100_query_traces --cache-header examples/data/spark/cache \
--mode w2v --vec-size 32 --epochs 50 --alpha 0.025
# via doc2vec 
python examples/data/spark/1.operator_encoding.py -b TPCH --scale-factor 100 \
--src-path-header resources/dataset/tpch_100_query_traces --cache-header examples/data/spark/cache \
--mode d2v --vec-size 32 --epochs 50 --alpha 0.025
```

2. get query-level tabular features
```bash
export PYTHONPATH="$PWD"
python examples/data/spark/2.sql_struct_extraction.py -b TPCH --scale-factor 100 \
--src-path-header resources/dataset/tpch_100_query_traces --cache-header examples/data/spark/cache
```

3. get operator cbo features
```bash
python examples/data/spark/3.oplan_extraction.py
```

4. get additional feats to enable QF
```bash
python examples/data/spark/4.qf_addition.py
```